# -*- coding: utf-8 -*-
"""Project A5 Big Baby.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15WmNmd4ck5wyrbj2v_TMkFhfTzFPY17e

# Cleaning data
"""

import pandas as pd

!wget https://github.com/Ratsemaat/Macrosomia/raw/main/Pregnancy_Data.csv -O Pregnancy_Data.csv
df = pd.read_csv("Pregnancy_Data.csv")
df['bmi'] = df['bmi'].str.replace(',','.')
df=df.astype({'bmi': 'float64'})

"""Dropping irrelevant data"""

df = df.drop("A", axis=1) # some kind of ID
df = df.drop("ga_week", axis=1) 
df = df.drop("idcode", axis=1) # We do not need to identify mothers.
df = df.drop("kmiyle25", axis=1) # There already exists BMI column. So a boolean that tells whether it is higher than 25 is useless.

#Since the timing of glucose tolerance test does not matter.
df = df.drop("fasting_glycose", axis=1)
df = df.drop("gestage_gtt_ndalates", axis=1)
df = df.drop("gestage_gtt1", axis=1)
df = df.drop("gestage_gtt2_ndalates", axis=1)
df = df.drop("gestage_gtt2", axis=1)

#We already have difference of days, so we do not need to record any dates.
df = df.drop("synnitus", axis=1)
df = df.drop("datekuni20", axis=1)
df = df.drop("datekuni24", axis=1)
df = df.drop("datekuni28", axis=1)
df = df.drop("datekuni34", axis=1)
df = df.drop("dateviimane", axis=1)
df = df.drop("pikkus", axis=1) # Height is taken into account in BMI column.

#The difference in weights is duplicated a lot
df = df.drop("iive2034", axis=1)
df = df.drop("iive2434", axis=1)
df = df.drop("kaaluiivekuni34", axis=1)
df = df.drop("kaaluiivekuni28", axis=1)
df = df.drop("iive2028", axis=1)
df = df.drop("kaaluiivekuni24", axis=1)
df = df.drop("iive2024", axis=1)
df = df.drop("iive2428", axis=1)
df = df.drop("iive2834", axis=1)
df = df.drop("iive_34_kuni_hiljemkui34", axis=1)
df = df.drop("iive_28_hiljemkui34", axis=1)
df = df.drop("kaaluive_alla20", axis=1)
df = df.drop("iive_24_hiljemkui34", axis=1)
df = df.drop("iive_20hiljemkui34", axis=1)
df = df.drop("iive_alg_kuni_hiljemkui34", axis=1)
df = df.drop("kaaluiivekuniviimasekaaluni", axis=1)

# `synnitus_ras_kestus_paevades` and `gestage` are equal (except for 1 row)
df = df.drop("gestage", axis=1)

"""# Data preprocessing

Renaming and translating.
"""

df =df.rename(columns={"synnitus_ras_kestus_paevades": "gestage_total",
                   "vanus_algul": "age",
                   "mitmes_syn":"childbirth_n",
                   "gdm_varem":"prev_gdm",
                   "suur_laps_varem":"prev_macrosomia",
                   "sugu":"sex",
                   "synnikaal":"child_weight",
                   "makrosoomia":"macrosomia",
                   "kaal_enne":"weight_start",
                   "maxtotalkaal":"max_weight",
                   "kaaluiive":"max_weight_delta",
                   "gestagekuni20": "gestage_days_20",
                   "gestagekuni24": "gestage_days_24",
                   "gestagekuni28": "gestage_days_28",
                   "gestagekuni34": "gestage_days_34",
                   "gestage_viimane":"gestage_days_last",
                   "kaal_kuni20": "weight_20",
                   "kaal_kuni24": "weight_24",
                   "kaalkuni28": "weight_28",
                   "kaalkuni34": "weight_34",
                   "kaal_viimane_enne synnitust": "weight_last",
                   })

print("There are " + str(df.shape[1]) + " basic attributes in total.")

df['macrosomia'].value_counts()

for field in df.columns:
    if df[field].dtype=='int64' or df[field].dtype=='float64':
        df.hist(column=field)

df[df.weight_20.isna() | df.weight_24.isna() | df.weight_28.isna() | df.weight_34.isna()]
# Quite a lot missing weight values

df[df.weight_20.isna() & df.weight_24.isna() & df.weight_28.isna() & df.weight_34.isna()]



df[df.macrosomia == 1].child_weight.hist(bins=20)

df[df.macrosomia == 0].child_weight.hist(bins=20)

df

# Replace missing weights using interpolate

df[['weight_start', 'weight_20', 'weight_24', 'weight_28', 'weight_34', 'weight_last']] = df[['weight_start', 'weight_20', 'weight_24', 'weight_28', 'weight_34', 'weight_last']].interpolate(axis=1)

# Some feature engineering

df['weight_20_gain'] = (df['weight_20'] - df['weight_start'])
df['weight_24_gain'] = (df['weight_24'] - df['weight_20'])
df['weight_28_gain'] = (df['weight_28'] - df['weight_24'])
df['weight_34_gain'] = (df['weight_34'] - df['weight_28'])
df['weight_last_gain'] = (df['weight_last'] - df['weight_34'])

df['relative_max_weight_gain'] = (df['max_weight'] - df['weight_start'])/df['weight_start']
df['relative_weight_20_gain'] = (df['weight_20'] - df['weight_start'])/df['weight_start']
df['relative_weight_24_gain'] = (df['weight_24'] - df['weight_20'])/df['weight_20']
df['relative_weight_28_gain'] = (df['weight_28'] - df['weight_24'])/df['weight_24']
df['relative_weight_34_gain'] = (df['weight_34'] - df['weight_28'])/df['weight_28']
df['relative_weight_last_gain'] = (df['weight_last'] - df['weight_34'])/df['weight_34']

df['relative_max_weight_gain'].hist(bins=20)

df['relative_weight_28_gain'].hist(bins=20)

df['relative_weight_34_gain'].hist(bins=20)

"""Meanings of different fields:

- **bmi** - body mass index
- **gdm** - whether the person had gestational diabetes
- **gdm_jm** - 0 if the person had low risk of GDM and was not tested for GDM; 1 if the person had risk of GDM, but was not tested; 2 if the person was tested, but did not have GDM; 3 if the person was tested and had GDM
- **age** - age of the person in years
- **childbirth** - how many times the person has given birth before
- **prev_gdm** - whether the person has had GDM before
- **prev_macrosomia** - whether the person has had a child with macrosomia before
- **gestage_total** - duration of pregnancy in days
- **sex** - sex of the child
- **child_weight** - weight of the child in grams
- **macrosomia** - whether the child has macrosomia
- **weight_start** - weight of the person in kg at the start of pregnancy
- **max_weight** - the maximum weight of the person in kg during the pregnancy
- **max_weight_delta** - *max_weight* - *weight_start* in kg
- **relative_max_weight_gain** - $\frac{max\_weight - weight\_start}{weight\_start}$
- **relative_weight_20_gain** $\frac{weight\_start - weight\_20}{weight\_20}$
- **relative_weight_24_gain** $\frac{weight\_24 - weight\_20}{weight\_20}$
- **relative_weight_28_gain** $\frac{weight\_28 - weight\_24}{weight\_24}$
- **relative_weight_34_gain** $\frac{weight\_34 - weight\_28}{weight\_28}$
- **relative_weight_last_gain** $\frac{weight\_last - weight\_34}{weight\_34}$
- **weight_20**, **weight_24**, **weight_28**, **weight_34** - weight of the person in kg at week 20, 24, 28, 34
- **weight_last** - last measured weight during pregnancy in kg

"""

# There are NAN values in this column
# It means that it is the first child.
# Since these are nominal attributes, we use 1-hot encoding
df = pd.get_dummies(df, columns=['prev_gdm', 'gdm', 'gdm_jm'])

df.max_weight_delta=df.max_weight-df.weight_start

for column in df.columns:
  print(column + "  " + str(df[column].isna().sum()))

"""# Training

Splitting data into training, testing and validation dataset
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df.drop("macrosomia", axis=1), df["macrosomia"], test_size=0.3, random_state=0)

X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=0)

"""Training different models"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.utils import class_weight
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import numpy as np
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import roc_auc_score
import sklearn

def train_models(X_train: pd.DataFrame, y_train: pd.DataFrame, x_features, cross_validate=True)-> pd.DataFrame:
  dfs = []
  models = [
          ('RF', RandomForestClassifier()),
          ('KNN', KNeighborsClassifier()),
          ('SVM', SVC(probability=True)), 
          ('GNB', GaussianNB()),
          ('XGB', XGBClassifier())
        ]
  results = []
  names = []
  scoring = ['f1', 'roc_auc', 'recall']

  validation_results = pd.DataFrame(columns=['model', 'accuracy', 'f score', 'recall', 'precision', 'roc auc'])
  trained_models = []

  for name, model in models:
    if cross_validate:
      kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)
      cv_results = model_selection.cross_validate(model, X_train[x_features], y_train, cv=kfold, scoring=scoring)

      results.append(cv_results)
      names.append(name)

      this_df = pd.DataFrame(cv_results)
      this_df['model'] = name
      dfs.append(this_df)
    
    # Create new model for measuring how good model is on validation data
    new_model = sklearn.base.clone(model)
    new_model.fit(X_train[x_features], y_train)
    y_pred = new_model.predict(X_validation[x_features])
    model_data = {
        'model': name,
        'accuracy': accuracy_score(y_validation, y_pred),
        'f score': f1_score(y_validation, y_pred),
        'recall': recall_score(y_validation, y_pred),
        'precision': precision_score(y_validation, y_pred),
        'roc auc': roc_auc_score(y_validation, new_model.predict_proba(X_validation[x_features])[:, 1])
    }
    validation_results = validation_results.append(model_data, ignore_index=True)
    trained_models.append((name, new_model))

  if cross_validate:
    final = pd.concat(dfs, ignore_index=True)
    print('Results with cross validation')
    print(final)

  print('Results on validation data')
  print(validation_results)

  return trained_models

case1_features = ["bmi", "gdm_0", "gdm_1", "age", "childbirth_n", "prev_macrosomia", "sex", "weight_start", "max_weight_delta"]

t = train_models(X_train, y_train, case1_features)
print(t[0][1].feature_importances_)

features_2 = ["bmi", "gdm_0", "gdm_1", "gdm_jm_0", "gdm_jm_1", "gdm_jm_2", "gdm_jm_3", "age", "childbirth_n", "prev_macrosomia", "sex", "weight_start", "max_weight_delta", "prev_gdm_0.0", "prev_gdm_1.0", "gestage_total"]
trained = train_models(X_train, y_train, features_2)

def undersample_data(X_train, y_train, factor):
  np.random.seed(0)

  negative_indices_train = y_train[y_train == 0].index
  positive_count_train = len(y_train[y_train == 1])
  negative_undersampling_indices_train = np.random.choice(negative_indices_train, min(int(positive_count_train * factor), len(negative_indices_train)), replace=False)
  positive_undersampling_indices_train = y_train[y_train == 1].index.to_numpy()
  undersampling_indices_train = np.concatenate([negative_undersampling_indices_train, positive_undersampling_indices_train])

  np.random.shuffle(undersampling_indices_train)

  X_train_undersampling = X_train.loc[undersampling_indices_train]
  y_train_undersampling = y_train.loc[undersampling_indices_train]

  return (X_train_undersampling, y_train_undersampling)

# Undersampling using case1features
for f in [0.9, 1, 1.2, 1.5, 2, 3]:
  print(f'Results when there can be at most {f} times more negative cases in the data than positive ones:')

  X_train_undersampling, y_train_undersampling = undersample_data(X_train, y_train, f)

  train_models(X_train_undersampling, y_train_undersampling, case1_features, cross_validate=False)

trained = []
# Undersampling using features_2
for f in [0.9, 1, 1.2, 1.5, 2, 3]:
  print(f'Results when there can be at most {f} times more negative cases in the data than positive ones:')

  X_train_undersampling, y_train_undersampling = undersample_data(X_train, y_train, f)

  trained.append(train_models(X_train_undersampling, y_train_undersampling, features_2, cross_validate=False))

# RF from previous block achieved the largest F score (0.37), test it on test data

rf = trained[1][0][1]
importances = sorted(rf.feature_importances_, reverse=True)
print(importances)

# Print the importances so that they can be put on the poster
for i in range(len(importances)):
  print(f'"{features_2[i]}",{importances[i]}')

pred = rf.predict(X_test[features_2])
print(f'F1 score on test data: {f1_score(y_test, pred)}')
print(f'Recall on test data: {recall_score(y_test, pred)}')
print(f'Precision on test data: {precision_score(y_test, pred)}')

# Models using data about weight gain
gain_features = [
                 "bmi",
                 "gdm_0",
                 "gdm_1",
                 "gdm_jm_0",
                 "gdm_jm_1",
                 "gdm_jm_2",
                 "gdm_jm_3",
                 "age",
                 "childbirth_n",
                 "prev_macrosomia",
                 "sex",
                 "weight_start",
                 "max_weight_delta",
                 "prev_gdm_0.0",
                 "prev_gdm_1.0",
                 "gestage_total",
                 "relative_max_weight_gain",
                 "relative_weight_20_gain",
                 "relative_weight_24_gain",
                 "relative_weight_28_gain",
                 "relative_weight_34_gain",
                 "relative_weight_last_gain",
                 "weight_20_gain",
                 "weight_24_gain",
                 "weight_28_gain",
                 "weight_34_gain",
                 "weight_last_gain"
]

X_train_undersampling, y_train_undersampling = undersample_data(X_train, y_train, 1)

trained = train_models(X_train_undersampling, y_train_undersampling, gain_features, cross_validate=False)

# Now try training with original data that is not undersampled
trained = train_models(X_train, y_train, gain_features)
trained[0][1].feature_importances_